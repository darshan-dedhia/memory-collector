{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Memory Collector","text":"<p>A Kubernetes-native collector for monitoring memory subsystem interference between pods. This project is under active development and we welcome contributors to help build this critical observability component.</p>"},{"location":"#overview","title":"Overview","text":"<p>Memory Collector helps Kubernetes operators identify and quantify performance degradation caused by memory subsystem interference (\"noisy neighbors\") by collecting metrics about:</p> <ul> <li>Memory bandwidth utilization</li> <li>Last Level Cache (LLC) usage</li> <li>CPU performance counters related to memory access</li> </ul> <p>This data helps operators: - Identify when pods are experiencing memory subsystem interference - Quantify the performance impact of noisy neighbors - Build confidence before deploying memory interference mitigation solutions</p>"},{"location":"#why-this-matters","title":"Why This Matters","text":"<p>Memory subsystem interference can cause: - 25%+ increase in cycles per instruction (CPI) - 4x-13x increase in tail latency - Reduced application performance even with CPU and memory limits</p> <p>Common sources of interference include: - Garbage collection - Big data analytics - Security scanning - Video streaming/transcoding - Container image decompression</p>"},{"location":"#development-status-contributing","title":"Development Status &amp; Contributing","text":"<p>The project is in active development across several areas:</p>"},{"location":"#core-metrics-collection","title":"Core Metrics Collection","text":"<ul> <li>Implementing collection for Intel RDT and AMD QoS</li> <li>Collecting hardware performance counters: cycles, instructions, cache misses</li> <li>Defining Prometheus metrics</li> </ul>"},{"location":"#kubernetes-integration","title":"Kubernetes Integration","text":"<ul> <li>Helm chart, DaemonSet implementation</li> <li>Prometheus integration</li> </ul>"},{"location":"#testing-documentation","title":"Testing &amp; Documentation","text":"<ul> <li>Architecture documentation</li> <li>Benchmark suite with example workloads</li> <li>Integration testing framework</li> </ul>"},{"location":"#get-involved","title":"Get Involved","text":"<p>We welcome contributions! Here's how you can help:</p> <ul> <li>Code: Check our Good First Issues and Development Guide</li> <li>Use Cases: Share interference scenarios, test in your environment</li> <li>Discussion: Open GitHub Issues or email yonch@yonch.com</li> <li>Schedule a chat: https://yonch.com/collector</li> </ul>"},{"location":"#project-background","title":"Project Background","text":"<p>This project builds on research and implementation from: - Google's CPI\u00b2 system - Meta's Resource Control implementation - Alibaba Cloud's Alita system - MIT's Caladan project</p>"},{"location":"#license","title":"License","text":""},{"location":"#code","title":"Code","text":"<p>Licensed under the Apache License, Version 2.0</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"cmt-perf/","title":"How to collect CMT measurements with Perf","text":""},{"location":"cmt-perf/#intel-cmt-cat-summary","title":"intel-cmt-cat: summary","text":"<p>The <code>perf_monitoring.c</code> file:</p> <ul> <li>Checks if perf is available by checking if <code>/proc/sys/kernel/perf_event_paranoid</code> exists.</li> <li>Checks if RDT exists by reading <code>/sys/devices/intel_cqm/type</code><ul> <li>if it exists, its value (as integer) is the perf <code>type</code> field</li> <li>traverses <code>/sys/devices/intel_cqm/events</code> for events <code>llc_occupancy</code>, <code>local_bytes</code>, <code>total_bytes</code></li> <li>their value is parsed to get the <code>config</code> field of the perf struct</li> <li>the same file with extension <code>.scale</code> is used to read a <code>double</code> scale</li> </ul> </li> </ul>"},{"location":"cmt-perf/#using-the-perf-command-line","title":"Using the perf command line","text":"<p>Here are references from the web for monitoring RDT using perf</p> <p>A 2017 forum post was able to view events with <code>perf stat</code> as events:</p> <p><code>intel_cqm/llc_occupancy , intel_cqm/llc_local_bytes/,intel_cqm_total_bytes/</code></p> <p>(the last value seems to have a typo replacing <code>/</code> with <code>_</code>)</p> <p>An Intel/Kanaka Juvva presentation at LinuxCon'2015 shows per-application memory bandwidth monitoring with <code>perf</code> (slide 11):</p> <p>Two perf events are exported to userland - LOCAL_BW   - perf stat \u2013e intel_cqm/llc_local_bw/ -a \u201cmy_application\u201d - TOTAL_BW   - perf stat \u2013e intel_cqm/llc_total_bw/ -a \u201cmy_application\u201d</p> <p>A 2016 Kanaka Juuva presentation: - further mentions LLC Occupancy - shows memory bandwidth benchmark results - shows more process-based CLI examples, by PID:</p> <ul> <li>LLC_OCCUPANCY</li> <li>perf stat \u2013e intel_cqm/llc_occupancy/ -p \u201cpid of my_application\u201d</li> <li>discusses cgroups-based measurements. This might have been before the switch from cgroup to resctrl.</li> </ul>"},{"location":"cmt-perf/#appendix-a-journey-through-intel-cmt-cat","title":"Appendix: A journey through intel-cmt-cat","text":"<p>The intel-cmt-cat repo documentation suggests perf can read CMT data as well (table 5 in README).</p> <p>In this section, we look into how intel-cmt-cat uses perf, and document its usage so we can support that alongside the other counters.</p> <p>We start with the <code>pqos</code> CLI tool. Its command line parameters set up calls into the library in <code>lib/</code>:</p> <ul> <li><code>main</code> calls <code>selfn_monitor_cores</code> on the <code>-m</code> command line option.</li> <li><code>parse_monitor_cores</code> parses the <code>-m</code> command line option.</li> <li><code>parse_monitor_group</code> parses a string from the command line to a list of cores or pids, and calls <code>grp_add</code> on each.</li> <li><code>grp_add</code> allocates a <code>struct mon_group</code> called <code>new_grp</code> on the stack, then adds the core/pid/channel/etc. to the group using <code>grp_set_*</code>, and then appends it to a global variable <code>sel_monitor_group</code>.</li> <li>later, <code>main</code> calls <code>monitor_setup</code></li> <li><code>monitor_setup</code> calls the library API depending on the type of monitor. For cores, it calls <code>pqos_mon_start_cores</code>.</li> </ul> <p>Going into the library:</p> <ul> <li><code>pqos_mon_start_cores</code> calls <code>pqos_mon_start_cores_ext</code> (which also has an opt parameter)</li> <li><code>pqos_mon_start_cores_ext</code> checks input validity and then makes an <code>API_CALL(mon_start_cores...)</code></li> <li><code>API_CALL</code> is a macro that accesses a virtual table of monitoring operations called <code>api</code> in <code>api.c</code>. <ul> <li>This <code>api</code> variable is initialized in <code>api_init</code> to either the OS interface or MSR interface (these are mentioned in the repo's README).</li> <li>In the OS interface, the <code>mon_start_cores</code> function pointer is initialized to point to <code>os_mon_start_cores</code>.</li> </ul> </li> <li><code>os_mon_start_cores</code> validates the input, the available monitoring capabilities, and ensures the monitoring hadn't already started, and calls <code>os_mon_start_events</code>.</li> <li><code>os_mon_start_events</code>:<ul> <li>runs <code>perf_mon_is_event_supported</code> on every event, and if so, calls <code>perf_mon_start</code>.</li> <li>otherwise, checks <code>resctrl_mon_is_event_supported</code> and if so performs <code>resctrl_mon_start</code>.</li> </ul> </li> </ul> <p>Let's explore the flow that checks perf for supported events:</p> <ul> <li><code>perf_mon_is_event_supported</code> calls <code>get_supported_event</code>.</li> <li><code>get_supported_event</code> looks up the event in a global <code>events_tab</code>.<ul> <li>the first event in <code>events_tab</code> is <code>llc_occupancy</code>.</li> </ul> </li> </ul> <p>Initialization of perf monitoring in <code>perf_mon_init</code>:</p> <ul> <li>if <code>/proc/sys/kernel/perf_event_paranoid</code> exists, enables the PMU events (cycles, instructions, IPC, LLC misses, LLC references).</li> <li><code>set_arch_event_attrs</code> sets the <code>attr</code> field on PMU events. The <code>attr</code> field is a <code>struct perf_event_attr</code> (from the linux API).</li> <li><code>set_mon_type</code> reads <code>/sys/devices/intel_cqm/type</code> as an integer into the global variable <code>os_mon_type</code>. This int is then used in the perf attr as its <code>type</code> field in <code>set_rdt_event_attrs</code>.</li> <li><code>set_mon_events</code> then traverses the directory <code>/sys/devices/intel_cqm/events</code>. <ul> <li>For each file, it tries to find an entry in <code>events_tab</code> whose <code>name</code> field is the same as the file name. </li> <li>For every match, it calls <code>set_rdt_event_attrs</code>.</li> </ul> </li> <li><code>set_rdt_event_attrs</code><ul> <li>reads the file</li> <li>assumes the contents has a <code>=</code>, discards everything before the first <code>=</code> and parses the rest as an integer. this will be <code>attrs.config</code></li> <li>reads another file filename+<code>.scale</code> suffix</li> <li>parses it as a double. this will be the event's <code>scale</code></li> </ul> </li> </ul> <p>So far, we covered initialization and checking event availability. Now let's see how the library configures the kernel to start monitoring:</p> <ul> <li><code>perf_event_open</code> makes the syscall via <code>syscall(__NR_perf_event_open, attr, pid, cpu, group_fd, flags)</code></li> </ul>"},{"location":"collection/","title":"Memory Collector Telemetry Strategy","text":""},{"location":"collection/#overview","title":"Overview","text":"<p>Our telemetry strategy prioritizes high-resolution, low-level data collection to build a foundation for understanding memory subsystem interference. By focusing on simplicity and data quality in the initial collector, we can enable rapid iteration and validation of detection algorithms.</p> <p>The key aspects of our approach are:</p> <ul> <li>Collect per-process, per-core metrics at 1 millisecond granularity to capture interference at a meaningful timescale</li> <li>Collect per-process cache occupancy metrics at 1 millisecond granularity</li> <li>Generate synchronized datasets for joint analysis</li> <li>Implement in stages to manage complexity</li> </ul> <p>This \"firehose\" telemetry will enable us to build a dataset for offline analysis, allowing us to identify patterns and develop algorithms for real-time interference detection.</p>"},{"location":"collection/#telemetry-collection","title":"Telemetry Collection","text":"<p>The collector will monitor and record the following metrics for each process at 1 millisecond granularity:</p> <ul> <li>Process ID</li> <li>Core ID </li> <li>Core frequency during the measured interval</li> <li>Cycles </li> <li>Instructions</li> <li>Last level cache misses</li> </ul> <p>Modern cloud environments routinely run dozens or even hundreds of applications on a single server, each with its own dynamic memory usage patterns. In an extreme case, with 100 applications changing phase every second on average, there would be a phase change every 10 milliseconds in aggregate.</p> <p>The 1 millisecond telemetry granularity enables us to detect this behavior and characterize interference at a meaningful timescale.</p> <p>In addition to these per-process metrics, we will also collect cache occupancy measurements using Intel RDT's Cache Monitoring Technology (CMT) or an equivalent mechanism. This data will be collected per process at the same 1 millisecond granularity.</p> <p>Monitoring cache usage per process is necessary because caches maintain state across context switches and are shared by all threads of a process.</p>"},{"location":"collection/#data-format","title":"Data Format","text":"<p>For the initial version, telemetry will be written to CSV files to simplify data collection and analysis. Each row will represent a single measurement interval for a specific process.</p> <p>We will generate two datasets:</p> <ol> <li>Per-process, per-core measurements (process ID, core ID, frequency, cycles, instructions, LLC misses)</li> <li>Per-process cache occupancy measurements</li> </ol> <p>While these datasets will be separate, they will be synchronized and aligned by timestamp to enable joint analysis.</p>"},{"location":"collection/#implementation-stages","title":"Implementation Stages","text":"<p>To manage complexity, we will implement telemetry collection in two stages:</p> <ol> <li>Collect per-process, per-core measurements (process ID, core ID, frequency, cycles, instructions, LLC misses)</li> <li>Add per-process cache occupancy measurements using Intel RDT or an equivalent mechanism</li> </ol> <p>This staged approach allows us to validate the core telemetry pipeline before adding the complexity of cache monitoring.</p> <p>For the cache monitoring stage, we will need to assign each process a unique identifier (e.g., CLOS for Intel RDT) to track its cache usage. This will require additional system-level coordination and metadata management.</p>"},{"location":"collection/#analysis-and-algorithm-development","title":"Analysis and Algorithm Development","text":"<p>By collecting high-resolution telemetry from multiple clusters, both real-world deployments and benchmark environments, we aim to build a representative dataset capturing a wide range of interference scenarios.</p> <p>Analyzing this data offline using big data techniques will help us identify common interference patterns, resource usage signatures, and relevant metrics for detecting contention.</p> <p>These insights will inform the development of algorithms for real-time interference detection in future collector versions. Starting with a thorough understanding of low-level behavior is key to building effective higher-level detection and mitigation strategies.</p>"},{"location":"devlog/","title":"Devlog","text":"<p>Documentation of development steps, environment, and dependencies  </p> <ul> <li>Contributors: atimeofday</li> <li>Goals: Create skeleton collector with Prometheus endpoint</li> <li>Issues: https://github.com/perfpod/memory-collector/issues/19</li> </ul> <p>Initial environment and tools:</p> <pre><code># Shell: Bash\ndistrobox create --image fedora:40 --name memory-collector \ndistrobox enter memory-collector\nsudo dnf install git go\n\n# cd to preferred project directory\n# Clone (fork of) project\ngit clone https://github.com/perfpod/memory-collector\ncd memory-collector\n</code></pre> <p>Issue 19 objective 1: Create a <code>main.go</code> file in <code>cmd/collector</code></p> <pre><code>mkdir -p cmd/collector\ncd cmd/collector\ntouch main.go\n</code></pre> <ul> <li>Prometheus client_golang reference guide: https://prometheus.io/docs/guides/go-application/</li> <li>Go package installation reference: https://go.dev/doc/go-get-install-deprecation</li> <li>Go Module reference: https://go.dev/ref/mod#go-mod-init</li> <li><code>go get</code> and <code>go install</code> require a Go Module and/or @version tag as of Go 1.17 in August 2021</li> <li>Prometheus go_client installation instructions appear to be outdated and missing a piece</li> <li>Submitted issue to Prometheus documentation repository: https://github.com/prometheus/docs/issues/2556#issue-2736636166</li> <li>Proceeded with Prometheus client_golang guide </li> </ul> <pre><code>cd cmd/collector\ngo mod init memory-collector\ngo get github.com/prometheus/client_golang/prometheus\ngo get github.com/prometheus/client_golang/prometheus/promauto\ngo get github.com/prometheus/client_golang/prometheus/promhttp\n</code></pre> <p>Issue 19 objective 2: Expose an endpoint on a known fixed port </p> <pre><code># Wrote and tested example Go exposition application from Prometheus guide\ngo run main.go &amp;\ncurl http://localhost:2112/metrics\n</code></pre> <p>Issue 19 objective 3: Expose the <code>up</code> metric with value 1</p> <pre><code>// Created, registered, and set an 'up' metric in func main()\n\nupMetric := prometheus.NewGauge(prometheus.GaugeOpts{\n    Namespace:  \"perfpod\",\n    Subsystem:  \"memory_collector\",\n    Name:       \"up_metric\",\n    Help:       \"Test metric to confirm skeleton application functionality.\",\n})\nprometheus.MustRegister(upMetric)\n\nupMetric.Set(1)\n</code></pre> <p>Issue 19 objective 4: Manually verify: query the endpoint using <code>curl</code> or <code>wget</code></p> <pre><code>curl -s http://localhost:2112/metrics | grep up_metric\n</code></pre> <p>Output:</p> <pre><code># HELP perfpod_memory_collector_up_metric Test metric to confirm skeleton application functionality.\n# TYPE perfpod_memory_collector_up_metric gauge\nperfpod_memory_collector_up_metric 1\n</code></pre> <p>Issue 19 objective 5: Move the code into a function (not <code>main()</code>)</p> <pre><code>// Moved Up metric into \"func recordMetrics()\" and added function call in main()\n\nfunc main() {\n    recordMetrics()\n\n    http.Handle(\"/metrics\", promhttp.Handler())\n    http.ListenAndServe(\":2112\", nil)\n}\n\n// Repeated manual verification endpoint query\n</code></pre> <p>Issue 19 objective 6: Add an integration test that verifies the metrics are up, using client_golang's testutil - TO DO - May require assistance</p> <ul> <li>Issue 19 split into 5/5 done and new Issue 20</li> <li>Issue 19 5/5 PR opened and merged</li> </ul> <ul> <li>Contributors: atimeofday</li> <li>Goals: Add integration test to Prometheus endpoint</li> <li>Issues: https://github.com/perfpod/memory-collector/issues/20</li> </ul> <p>Research &amp; references:</p> <pre><code>https://go.dev/doc/tutorial/add-a-test\nhttps://albertmoreno.dev/posts/testing-prometheus-metrics-in-integration-tests-in-golang/\nhttps://github.com/prometheus/client_golang/blob/main/prometheus/testutil/testutil.go\nhttps://github.com/prometheus/client_golang/blob/main/prometheus/testutil/testutil_test.go\n</code></pre> <pre><code>go get github.com/prometheus/client_golang/prometheus/testutil \ngo get github.com/stretchr/testify/require\n</code></pre> <p>Go test format:</p> <pre><code>[filename]_test.go\n\nimport(\n    [...]\n)\n\nfunc [TestFunction](t *testing.T) {\n    // Set test values\n    // Perform test\n}\n\n// Perform more tests\n</code></pre> <ol> <li>Created skeleton test based on examples </li> </ol> <pre><code>func TestMetricsUp(t *testing.T) {\n    require.Eventuallyf(t, func() bool {\n\n        // Test values\n        // ??? expected format\n\n        if err := testutil.ScrapeAndCompare(serverURL+\"/metrics\", strings.NewReader(expected), metricName); err == nil {\n            return true\n        } else {\n            t.Log(err.Error())\n            return false\n        }\n    }, time.Second, 100*time.Millisecond, \"Could not find metric %s with value %d\", metricName, expectedMetricValue)\n}\n</code></pre> <ol> <li>Checked the implementation of the testutil ScrapeAndCompare function, and notably, the implementation of its own integration test.</li> <li>Located and implemented the exact input template required by the function, then implemented generalized code for the template.</li> <li>Researched goroutines to allow automatically initializing the (currently local) remote server to be tested.</li> </ol> <pre><code>go main()\ntime.Sleep(1 * time.Second)\n</code></pre> <ol> <li>Refined logical flow from example code for improved readability.</li> </ol> <ul> <li>Issue 20 done</li> </ul> <ul> <li>Contributors: </li> <li>Goals: </li> <li>Issues: </li> </ul>"},{"location":"architecture/decisions/2024-12-21-container-notifications/","title":"2024-12-21: Notifications for container lifecycle events","text":""},{"location":"architecture/decisions/2024-12-21-container-notifications/#status","title":"Status","text":"<p>Draft, WIP</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/#context","title":"Context","text":"<p>We'd like the collector to show how memory resource contention influences container performance.</p> <p>To do that, we'd need to monitor: 1. Resource contention - can do this with <code>resctrl</code>, or by monitoring LLC Misses using perf counters 2. Container performance - current plan is to do this by monitoring CPI (cycles per instruction)</p> <p>For CPI monitoring, we'd need to have an inventory of containers on the system, and correctly instrument them as they arrive/go. In this issue, we add a component to monitor the arrival and departure of containers in the system.</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/#options-considered","title":"Options considered","text":""},{"location":"architecture/decisions/2024-12-21-container-notifications/#kubelet-api","title":"Kubelet API","text":"<p>If we're focusing on Kubernetes, kubelet provides an HTTP API accessible locally. This appears to be an undocumented, unstable API, that is nevertheless available in kubelet.</p> <p>Stack overflow discussion points to a project kubeletctl. The referenced blog post shows several <code>curl</code> commands to interact with the API. According to the blog post, this is available because the default kubelet configuration allows for anonymous (unauthenticated) requests, so this relies on users not fortifying their systems to this vulnerability. The specific implementation in kubeletctl appears a thin implementation of HTTP calls, so it might be best to reimplement this in our on library rather than take a dependency.</p> <p>Pros: - Should provide metadata on Pods, not only containers - Does not rely on a specific container runtime (docker, containerd, etc.)</p> <p>Cons: - Undocumented, unstable API - Requires access to kubelet, which may not be available in all environments - Appears to require polling (no <code>watch</code>). If so, will react slowly and incur more overhead.</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/#filesystem-watch-on-the-cgroup-directory-eg-inotify","title":"Filesystem watch on the cgroup directory (e.g., <code>inotify</code>)","text":"<p>This is the method used by Koordinator.sh in its PLEG component. It watches the cgroup root path for each of the Kubernetes QoS classes, for new pod directories. A new pod directory adds that pod subdirectory to a container watcher, which then issues container events.</p> <p>Pros: - Does not require access to kubelet - Does not depend on a container runtime - ABI is stable and well-documented - Supports inotify, which is efficient and low-overhead</p> <p>Cons: - Does not provide metadata beyond the pod and container IDs</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/#cri-container-runtime-interface-events","title":"CRI (Container Runtime Interface) events","text":""},{"location":"architecture/decisions/2024-12-21-container-notifications/#kubernetes-api-ie-watching-the-control-plane","title":"Kubernetes API (i.e., watching the control plane)","text":""},{"location":"architecture/decisions/2024-12-21-container-notifications/#decision","title":"Decision","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/","title":"2025-01-02: AWS test runners","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#context","title":"Context","text":"<p>We'd like to run some tests on AWS to check the availability of PMC (Performance Monitoring Counters) and Linux resctrl on different instance types. To do this, we'll want an automated way to run tests on different instance types.</p> <p>As of writing, the main check will be <code>cpi-count</code>, which checks the availability of cycles and instructions, and compares the results of <code>go-perf</code> and <code>perf</code> to sanity-check the results. </p> <p>In the future, we'll want to add more tests and similarly run them on different instance types. For example:</p> <ul> <li>Checking other counters than cycles and instructions (e.g., LLCMisses)</li> <li>Checking the availability of <code>resctrl</code> in Linux</li> <li>Verifying <code>resctrl</code> is able to control memory bandwidth and cache allocation</li> </ul> <p>This decision is about individual, relatively simple checks that run on a single instance. Tests that require complex workloads (e.g., DeathStarBench) are out of scope for this decision.</p>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#options-considered-ec2-based","title":"Options considered - EC2 based","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#common-pros-and-cons","title":"Common pros and cons","text":"<p>Pros:</p> <ul> <li>Easy to run multiple instances</li> <li>Gives control over the operating system and AMI, if we need that control in the future.</li> <li>Few components running on the VM, so this is less noisy and more conducive to benchmarking.</li> </ul> <p>Cons:</p> <ul> <li>Only works on AWS. Will require adaptation for other clouds.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#aws-ec2-with-user-data","title":"AWS EC2 with User Data","text":"<p>This is the strawman: spin up an EC2 instance, install the necessary tools, run the tests, and then tear down the instance. User Data is a way to run commands when the instance is first launched.</p> <p>Additional pros:</p> <ul> <li>None</li> </ul> <p>Additional cons:</p> <ul> <li>There is no good way to get results out of the instance.</li> <li>It is hard to check when tests are done.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#aws-ec2-with-a-github-self-hosted-runner","title":"AWS EC2 with a GitHub Self-Hosted Runner","text":"<p>This spins up an EC2 instance that runs a GitHub Actions runner. The runner is labeled specifically for the test that spins it up. The Action then runs the test workflow on the runner it just spun up. At the end of the test, the workflow tears down the runner.</p> <p>Additional pros:</p> <ul> <li>Integrated well with GitHub Actions: natively extracts results and continues the workflow when the test is done.</li> </ul> <p>Additional cons:</p> <ul> <li>More complex than EC2 with User Data (but solves that approach's problems).</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#options-considered-kubernetes-based","title":"Options considered - Kubernetes based","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#common-pros-and-cons_1","title":"Common pros and cons","text":"<p>Pros:</p> <ul> <li>We might be able to reuse this infrastructure for benchmarks with complex Kubernetes workloads.</li> </ul> <p>Cons:</p> <ul> <li>Complex. Need to set up a Kubernetes cluster and all its tooling.</li> <li>Less control over the operating system and AMI.</li> <li>Kubernetes has more components running on the Node (e.g., kubelet) that introduce noise, so this approach is less conducive to benchmarking.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#spin-up-a-kubernetes-cluster-and-run-the-tests-in-a-pod","title":"Spin up a Kubernetes cluster and run the tests in a pod","text":"<p>This is the approach the Cilium uses for its EKS conformance tests..</p> <p>Additional pros:</p> <ul> <li>Easy to check for completion and extract results (with <code>kubectl</code>).</li> </ul> <p>Additional cons:</p> <ul> <li>More components to set up and tear down (the Kubernetes control plane) which increases the time it takes to run tests and the cost of running tests.</li> <li>Need to write the functionality to extract results ourselves.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#maintain-a-persistent-kubernetes-cluster-with-actions-runner-controller","title":"Maintain a persistent Kubernetes cluster with <code>actions-runner-controller</code>","text":"<p>following GitHub's \"Autoscaling with self-hosted runners\":</p> <ul> <li>Run a Kubernetes cluster on one of the cloud providers</li> <li>Use GitHub Actions to trigger tests</li> <li>Tests run self hosted on the Kubernetes cluster. The actions-runner-controller seems to be the official controller for this.</li> <li>Each test that requires a specific node type will trigger a runner that only runs on that node type</li> </ul> <p>I believe we can add a nodeSelector in the AutoscalingRunnerSet from the values.yaml when deploying the controller (under template.spec). So this might require a controller deployment per node type.</p> <p>Additional pros:</p> <ul> <li>Very little spin-up and tear-down code, as the controller handles the scaling. This reulsts in simpler Actions, and more reliable cleanup.</li> <li>Tests run on GitHub Runners, so they extract results natively.</li> <li>We can spin up similar clusters on other clouds, and reuse the exact same Actions to run the tests on other clouds' instance types.</li> </ul> <p>Additional cons:</p> <ul> <li>Cluster is relatively complex: needs to anticipate all instance types we want to test on, and add controllers for each. This can be implemented with for loops in a helm chart, but still adds complexity.</li> <li>Cluster would be persistent, so it has ongoing cost, regardless of whether tests are running or not.</li> <li>The cluster would be maintained separately from the tests, so it might be hard to keep them in sync.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#replicatedcom-compatibility-matrix","title":"Replicated.com Compatibility Matrix","text":"<p>It is a service that spins up full Kubernetes clusters for testing, and bills by usage.</p> <p>Additional pros:</p> <ul> <li>Easy to spin up and tear down clusters.</li> <li>Support for AWS, GCP, Azure, OCI, as well as Openshift, RKE2, and k3s.</li> <li>Might have credits for open source projects (at least with Openshift)</li> </ul> <p>Additional cons:</p> <ul> <li>Needs Kubernetes tooling installed (which complicates the Github Action)</li> <li>Markup over using the clouds directly (although it is small)</li> <li>No spot instance support</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#decision","title":"Decision","text":"<p>We'll use the EC2 + GitHub Actions Runner approach, because it is the simplest way that returns results and is easy to check for completion.</p>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#positive","title":"Positive","text":"<ul> <li>Can write the entire test as a GitHub Action.</li> <li>The same approach can be used for benchmarking.</li> <li>Can use AWS credits to run tests.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#negative","title":"Negative","text":"<ul> <li>We are currently just enabling AWS. To run on other clouds, the setup and cleanup would need to be updated.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#risks","title":"Risks","text":"<ul> <li>Making cleanup bulletproof would require iteration, which could lead to orphaned runners and their associated costs in the interim.</li> </ul>"},{"location":"ci/aws-setup/","title":"AWS Setup for CI Testing","text":"<p>Our CI system needs to test the collector on various AWS instance types to validate hardware-specific features. We use GitHub Actions to dynamically spin up EC2 instances with specific hardware configurations, run our tests, and then tear down the instances.</p>"},{"location":"ci/aws-setup/#overview","title":"Overview","text":"<p>The CI system uses two key community actions: - machulav/EC2-github-runner: Manages ephemeral EC2 instances for test execution - aws-actions/configure-aws-credentials: Handles AWS authentication through GitHub's OIDC provider</p> <p>Each test workflow creates a dedicated GitHub Actions runner on a fresh EC2 instance. This ensures our tests run in clean environments with specific hardware configurations.</p>"},{"location":"ci/aws-setup/#aws-account-setup","title":"AWS Account Setup","text":"<p>We maintain a dedicated AWS account for CI testing to isolate these resources from production environments. This separation provides clearer cost tracking and stronger security boundaries.</p>"},{"location":"ci/aws-setup/#administrative-access","title":"Administrative Access","text":"<p>After creating the dedicated CI testing account, set up administrative access:</p> <ol> <li>In the root account, navigate to IAM Identity Center</li> <li>Create a new user in the IAM Identity Center</li> <li>Create a Permission Set or use the existing \"Administrator Access\" permission set</li> <li>Note: \"Power User Access\" is insufficient as it doesn't allow IAM role creation</li> <li>Assign the user to the CI testing account with the Administrator Access permission set</li> </ol> <p>The Administrator Access permission set is required for subsequent IAM configuration steps. While Power User Access might seem sufficient, it lacks the necessary permissions for creating IAM roles needed for GitHub Actions integration.</p>"},{"location":"ci/aws-setup/#iam-configuration","title":"IAM Configuration","text":"<p>First, configure GitHub as an OIDC provider to enable secure authentication:</p> <ol> <li>Open the IAM console</li> <li>Navigate to \"Identity Providers\" and add a new provider</li> <li>Select \"OpenID Connect\"</li> <li>Use <code>https://token.actions.githubusercontent.com</code> as the provider URL</li> <li>Set the audience to <code>sts.amazonaws.com</code></li> </ol> <p>Next, create an IAM role for GitHub Actions:</p> <ol> <li>Create a new role</li> <li>Set the Trusted entity type to Web Identity</li> <li>Select the GitHub OIDC provider as the trust entity</li> <li>Fill in the org (<code>unvariance</code>) and repo (<code>collector</code>), not filling in the branch</li> <li>Add no permissions (we will do this in a moment)</li> <li>Name the role, e.g., <code>github-actions-collector</code>.</li> <li>Verify the generated trusted entities to be:</li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::&lt;ACCOUNT-ID&gt;:oidc-provider/token.actions.githubusercontent.com\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n                },\n                \"StringLike\": {\n                    \"token.actions.githubusercontent.com:sub\": \"repo:unvariance/collector:*\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"ci/aws-setup/#ec2-permissions","title":"EC2 Permissions","text":"<p>The IAM role needs permissions to manage EC2 instances and request Spot instances. Attach a policy with these minimum permissions:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:RunInstances\",\n                \"ec2:TerminateInstances\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceStatus\",\n                \"ec2:RequestSpotInstances\",\n                \"ec2:CancelSpotInstanceRequests\",\n                \"ec2:DescribeSpotInstanceRequests\",\n                \"ec2:DescribeSpotPriceHistory\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"ec2:CreateTags\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"ec2:CreateAction\": [\n                        \"RunInstances\",\n                        \"RequestSpotInstances\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"ci/aws-setup/#allowing-spot-instances","title":"Allowing Spot instances","text":"<p>According to AWS spot instance role docs:</p> <p>Under most circumstances, you don't need to manually create a service-linked role. Amazon EC2 creates the\u00a0AWSServiceRoleForEC2Spot\u00a0service-linked role the first time you request a Spot Instance using the console.</p> <p>And later:</p> <p>If you use the AWS CLI or an API to request a Spot Instance, you must first ensure that this role exists.</p> <p>So to enable Spot instances, we recommend making a request on the account using the console, which creates the role, and then canceling the request.</p>"},{"location":"ci/aws-setup/#instance-quotas","title":"Instance quotas","text":"<p>We found default EC2 and Spot quotas to be smaller than most machines that make the PMU available to VMs. We requested an increase to 192 cores.</p> <p>Quota requests can be made through this page. Note that there are separate quotas for On-Demand and for Spot.</p>"},{"location":"ci/aws-setup/#network-configuration","title":"Network Configuration","text":"<p>Create a dedicated VPC for CI testing:</p> <ol> <li>Create a new VPC with a single public subnet</li> <li>Set up appropriate security groups to allow:</li> <li>Outbound traffic on port 443 for communication with GitHub</li> </ol>"},{"location":"ci/aws-setup/#repository-variables","title":"Repository variables","text":"<p>Configure the repository with the following secrets that can be used in Actions:</p> <ul> <li><code>AWS_ROLE_ARN</code>: the ARN of the role that allows running and terminating instances</li> <li><code>AWS_REGION</code>: the region where we'll run runners</li> <li><code>AWS_SUBNET_ID</code>: the subnet ID, needs to be in <code>AWS_REGION</code></li> <li><code>AWS_SECURITY_GROUP_ID</code>: the name of the security group that allows runners to pull jobs</li> <li><code>REPO_ADMIN_TOKEN</code>: see below</li> </ul>"},{"location":"ci/aws-setup/#getting-a-token-for-ec2-github-runner","title":"Getting a token for ec2-github-runner","text":"<p>To register runners with GitHub, the <code>machulav/ec2-github-runner</code> action needs a GitHub token that has permissions to modify the the repository's set of self hosted runners. This might be transferable to user accounts but I haven't checked.</p> <p>A discussion thread implies that finer-grained permissions might be available, where a token would only be able to configure runners rather than full Administration privileges, but it didn't work.</p> <ol> <li>Configure your organization to allow fine-grained tokens. In Organization Settings -&gt; Third-party Access -&gt; Personal access tokens -&gt; Settings, allow access via fine-grained personal access tokens</li> <li>Create a fine-grained personal access token here: https://github.com/settings/personal-access-tokens/new</li> <li>Set the resource owner to be the organization</li> <li>Set the permission scope to \"Only select repositories\", and select the repo with the GitHub Action</li> <li>In Repository permissions, add \"Administration\" (read and write)</li> </ol>"},{"location":"ci/aws-setup/#github-workflow-configuration","title":"GitHub Workflow Configuration","text":"<p>For an example workflow, adapted from the ec2-github-runner README and configure-aws-credentials README example, see <code>/.github/workflows/aws-runner-template.yaml</code>.</p>"}]}